{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/py38/lib/python3.8/site-packages/pl_bolts/utils/semi_supervised.py:15: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"sklearn\", pypi_name=\"scikit-learn\")\n",
      "/root/miniconda3/envs/py38/lib/python3.8/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:35: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/root/miniconda3/envs/py38/lib/python3.8/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:93: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/root/miniconda3/envs/py38/lib/python3.8/site-packages/pl_bolts/losses/self_supervised_learning.py:234: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n",
      "/root/miniconda3/envs/py38/lib/python3.8/site-packages/pl_bolts/datamodules/experience_source.py:18: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"gym\")\n",
      "/root/miniconda3/envs/py38/lib/python3.8/site-packages/pl_bolts/datamodules/sklearn_datamodule.py:15: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"sklearn\")\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,json\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "from torch.utils.data import DataLoader,Dataset,Subset\n",
    "from importlib import import_module\n",
    "from IPython.display import display\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything,LightningDataModule,loggers,seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from pl_bolts.callbacks import PrintTableMetricsCallback\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.optim.swa_utils import AveragedModel, update_bn\n",
    "from torchmetrics.functional import accuracy\n",
    "from dataclasses import dataclass,asdict\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'test',\n",
       " 'sup_size': 128,\n",
       " 'unsup_size': 128,\n",
       " 'eval_size': 1000,\n",
       " 'dataset': 'cifar10',\n",
       " 'model': 'resnet1D',\n",
       " 'arch': 'fixmatch',\n",
       " 'label_ratio': 0.1,\n",
       " 'min_epochs': 50,\n",
       " 'max_epochs': 150,\n",
       " 'lr': 0.0003,\n",
       " 'threshold': 0.95,\n",
       " 'semi': True,\n",
       " 'num_workers': 8,\n",
       " 'auto_scale_batch_size': None,\n",
       " 'auto_lr_find': False,\n",
       " 'default_root_dir': '/autodl-tmp/checkpoints',\n",
       " 'log_dir': '/autodl-tmp/logs',\n",
       " 'dataset_dir': '/autodl-tmp/datasets',\n",
       " 'check_val_every_n_epochs': 1}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # custom config\n",
    "    name:str='test'\n",
    "    sup_size:int=128\n",
    "    unsup_size:int=128\n",
    "    eval_size:int=1000\n",
    "    dataset:str='cifar10'\n",
    "    model:str='resnet1D'\n",
    "    arch:str='fixmatch'\n",
    "    label_ratio:float=0.1\n",
    "    min_epochs:int=50\n",
    "    max_epochs:int=150\n",
    "    lr:float=3e-4\n",
    "    threshold:float=0.95\n",
    "    semi:bool=True\n",
    "    \n",
    "    # lightning config\n",
    "    num_workers:int=8\n",
    "    auto_scale_batch_size:str=None\n",
    "    auto_lr_find:bool=False\n",
    "    default_root_dir:str='/autodl-tmp/checkpoints'\n",
    "    log_dir:str='/autodl-tmp/logs'\n",
    "    dataset_dir:str='/autodl-tmp/datasets'\n",
    "    check_val_every_n_epochs:int=1\n",
    "    \n",
    "conf = Config()\n",
    "vars(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'test', 'sup_size': 1024, 'unsup_size': 1024, 'eval_size': 1000, 'dataset': 'cls10', 'model': 'resnet1d', 'arch': 'fixmatch', 'label_ratio': 0.1, 'min_epochs': 10, 'max_epochs': 20, 'lr': 0.1, 'semi': True, 'num_workers': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "conf=OmegaConf.load('conf/config.yaml')\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA():\n",
    "    @staticmethod\n",
    "    def copy(model:nn.Module,ema:nn.Module):\n",
    "        ema.load_state_dict(model.state_dict())\n",
    "        for name, param in model.named_parameters():\n",
    "            ema[name]=param.data.clone().detach_()\n",
    "    \n",
    "    @staticmethod\n",
    "    def update(model,ema_model,decay,global_step):\n",
    "        alpha = min(1 - 1 / (global_step +1), decay)\n",
    "        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "            ema_param.data.mul_(alpha).add_(1-alpha, param.data)\n",
    "\n",
    "class BaseModel(LightningModule):\n",
    "    def __init__(self,model,ema,methods,*args,**kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model=model\n",
    "        self.ema=ema\n",
    "        EMA.copy(self.model,self.ema)\n",
    "        self.methods=methods\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        EMA.update(self.model,self.ema,0.97,self.current_epoch)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=3e-4)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self.model(x)\n",
    "        ema_pred = self.ema(x)\n",
    "        val_loss = F.cross_entropy(pred, y)\n",
    "        ema_loss = F.cross_entropy(ema_pred, y)\n",
    "        self.log_dict({\"val_loss\": val_loss, \"ema_loss\": ema_loss})\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self.model(x)\n",
    "        ema_pred = self.ema(x)\n",
    "        test_loss = F.cross_entropy(pred, y)\n",
    "        ema_loss = F.cross_entropy(ema_pred, y)\n",
    "        self.log_dict({\"test_loss\": test_loss, \"ema_test_loss\": ema_loss})\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self.model(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from utils.image_aug import RandAugmentMC\n",
    "from utils.dataset import TransformWeakStrong as wstwice,TransformBaseWeakStrong as bwstwice\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader,Dataset,Subset\n",
    "    \n",
    "from torch.utils.data import DataLoader,random_split\n",
    "import torchvision as tv\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data,targets,transfom):\n",
    "        self.data=data\n",
    "        self.targets=targets\n",
    "        self.transfom=transfom\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        x,y=self.data[index],self.targets[index]\n",
    "        _x=self.transfom(x) if self.transfom else x\n",
    "        return _x,y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class TransformSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "channel_stats = dict(mean = [0.4914, 0.4822, 0.4465],\n",
    "                            std = [0.2023, 0.1994, 0.2010])\n",
    "eval_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(**channel_stats)\n",
    "        ])\n",
    "\n",
    "class BaseDataset(LightningDataModule):\n",
    "    def __init__(self,conf:Config) -> None:\n",
    "        super().__init__()\n",
    "        self.conf=conf\n",
    "        self.num_workers=8\n",
    "        \n",
    "    \n",
    "    def setup(self):\n",
    "        trainset_all = tv.datasets.CIFAR100(root=self.conf.dataset_dir, train=True, download=True)\n",
    "        testset = tv.datasets.CIFAR100(root=self.conf.dataset_dir, train=False, download=True,transform=eval_transform)\n",
    "        self.trainset,self.valset=random_split(trainset_all, [0.9,0.1])\n",
    "        self.testset=testset\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        weak = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Pad(2, padding_mode='reflect'),\n",
    "            transforms.RandomCrop(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(**channel_stats)\n",
    "        ])\n",
    "        strong = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Pad(2, padding_mode='reflect'),\n",
    "            transforms.RandomCrop(32),\n",
    "            RandAugmentMC(n=2, m=10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(**channel_stats)\n",
    "        ])\n",
    "        \n",
    "        unsup_transform = wstwice(weak, strong)\n",
    "        # split sup/unsup from trainset\n",
    "        _supset,_unsupset=random_split(self.trainset, [0.1,0.9])\n",
    "        supset=TransformSubset(_supset,weak)\n",
    "        unsupset=TransformSubset(_unsupset,unsup_transform)\n",
    "\n",
    "        # return dataloader\n",
    "        sup_loader=DataLoader(supset, batch_size=self.conf.sup_size, num_workers=self.num_workers, shuffle=True)\n",
    "        unsup_loader=DataLoader(unsupset, batch_size=self.conf.unsup_size, num_workers=self.num_workers, shuffle=True)\n",
    "        return [sup_loader,unsup_loader]\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valset, batch_size=self.conf.eval_size, num_workers=self.num_workers, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.testset, batch_size=self.conf.eval_size, num_workers=self.num_workers, shuffle=False)\n",
    "    \n",
    "trainset_all = tv.datasets.CIFAR100(root=conf.dataset_dir, train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lvhang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataset.py:342: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \"\n"
     ]
    }
   ],
   "source": [
    "torch.Tensor().float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_supset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_nodes': 1,\n",
       " 'auto_select_gpus': False,\n",
       " 'enable_progress_bar': True,\n",
       " 'max_steps': -1,\n",
       " 'log_every_n_steps': 50,\n",
       " 'sync_batchnorm': False,\n",
       " 'enable_model_summary': True,\n",
       " 'num_sanity_val_steps': 2,\n",
       " 'reload_dataloaders_every_n_epochs': 0,\n",
       " 'replace_sampler_ddp': True,\n",
       " 'detect_anomaly': False,\n",
       " 'amp_backend': 'native',\n",
       " 'move_metrics_to_cpu': False,\n",
       " 'multiple_trainloader_mode': 'max_size_cycle',\n",
       " 'inference_mode': True}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    logger= True\n",
    "    enable_checkpointing= True\n",
    "    callbacks= None\n",
    "    default_root_dir= None\n",
    "    gradient_clip_val= None\n",
    "    gradient_clip_algorithm= None\n",
    "    num_nodes: int = 1\n",
    "    num_processes= None,  # TODO: Remove in 2.\n",
    "    devices= None\n",
    "    gpus= None,  # TODO: Remove in 2.\n",
    "    auto_select_gpus: bool = False\n",
    "    tpu_cores= None,  # TODO: Remove in 2.\n",
    "    ipus= None,  # TODO: Remove in 2.\n",
    "    enable_progress_bar: bool = True\n",
    "    overfit_batches= 0.0\n",
    "    track_grad_norm= -1\n",
    "    check_val_every_n_epoch= 1\n",
    "    fast_dev_run= False\n",
    "    accumulate_grad_batches= None\n",
    "    max_epochs= None\n",
    "    min_epochs= None\n",
    "    max_steps: int = -1\n",
    "    min_steps= None\n",
    "    max_time= None\n",
    "    limit_train_batches= None\n",
    "    limit_val_batches= None\n",
    "    limit_test_batches= None\n",
    "    limit_predict_batches= None\n",
    "    val_check_interval= None\n",
    "    log_every_n_steps: int = 50\n",
    "    accelerator= None\n",
    "    strategy= None\n",
    "    sync_batchnorm: bool = False\n",
    "    precision= 32\n",
    "    enable_model_summary: bool = True\n",
    "    num_sanity_val_steps: int = 2\n",
    "    resume_from_checkpoint= None\n",
    "    profiler= None\n",
    "    benchmark= None\n",
    "    deterministic= None\n",
    "    reload_dataloaders_every_n_epochs: int = 0\n",
    "    auto_lr_find= False\n",
    "    replace_sampler_ddp: bool = True\n",
    "    detect_anomaly: bool = False\n",
    "    auto_scale_batch_size= False\n",
    "    plugins= None\n",
    "    amp_backend: str = \"native\"\n",
    "    amp_level= None\n",
    "    move_metrics_to_cpu: bool = False\n",
    "    multiple_trainloader_mode: str = \"max_size_cycle\"\n",
    "    inference_mode: bool = True\n",
    "    \n",
    "tc=TrainerConfig()\n",
    "\n",
    "vars(tc)\n",
    "asdict(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(conf:Config):\n",
    "    \n",
    "    callback_t=TQDMProgressBar()\n",
    "    callback_e=EarlyStopping(monitor='val_loss', min_delta=0.0, patience=5, verbose=False, mode='min', strict=True)\n",
    "    callback_p=PrintTableMetricsCallback()\n",
    "    callback_c=ModelCheckpoint(monitor='val_loss', save_top_k=1, save_last=True, save_weights_only=False, mode='min', dirpath=conf.default_root_dir, filename=conf.name+'{epoch:02d}-{val_loss:.2f}')\n",
    "    tb_logger = loggers.TensorBoardLogger(\n",
    "        save_dir=conf.log_dir,\n",
    "        name=conf.name\n",
    "    )\n",
    "    callbacks=[callback_e,callback_p,callback_c,callback_t]\n",
    "\n",
    "    dataset=import_module('datasets.'+conf.dataset)\n",
    "    model=import_module('models.'+conf.model)\n",
    "    arch=import_module('arch.'+conf.arch).Arch(model=model,**vars(conf))\n",
    "    trainer = Trainer(logger=tb_logger,callbacks=callbacks,**vars(conf))\n",
    "    trainer.fit(arch, dataset)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "utils.image_aug.RandAugmentMC"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.image_aug import RandAugmentMC\n",
    "RandAugmentMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 10]), torch.Size([5, 10]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b=torch.rand((5,10)),torch.rand((5,10))\n",
    "a.shape,b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset=tv.datasets.CIFAR100('/root/autodl-tmp/datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def uniform_split(dataset,length):\n",
    "    if isinstance(dataset,torch.utils.data.dataset.Subset):\n",
    "        # if is subset, restore subset to full-dataset while keep targets stay subset\n",
    "        indice=np.array(dataset.indices)\n",
    "        while isinstance(dataset,torch.utils.data.dataset.Subset):\n",
    "            dataset=dataset.dataset\n",
    "        targets=np.array(dataset.targets)[indice]\n",
    "        \n",
    "    data,targets=dataset.data,np.array(dataset.targets)\n",
    "    subsets=[]\n",
    "    unique_labels=np.unique(targets)\n",
    "    used_label_num=defaultdict(int)\n",
    "    for ratio in length:\n",
    "        # for every item in length, construct a subset\n",
    "        indice=[]\n",
    "        for y in unique_labels:\n",
    "            # for every label, append ratio*len idxes into indice\n",
    "            idxes=np.where(targets==y)[0]\n",
    "            cur=int(ratio*len(idxes))\n",
    "            used=used_label_num[y]\n",
    "            indice.extend(idxes[used:used+cur])\n",
    "            used_label_num[y]+=cur\n",
    "        subsets.append(Subset(trainset, indice))\n",
    "    return subsets\n",
    "a,b=uniform_split(trainset,[0.9,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.utils.data.dataset.Subset at 0x7fbdfb88cca0>,\n",
       " <torch.utils.data.dataset.Subset at 0x7fbbe9bc0c40>]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_targets,indices=np.array(a.dataset.targets),np.array(a.indices)\n",
    "sub_targets=full_targets[indices]\n",
    "unique_labels=np.unique(sub_targets)\n",
    "subsets=[]\n",
    "used_label_num=defaultdict(int)\n",
    "for ratio in [0.9,0.1]:\n",
    "    # for every item in length, construct a subset\n",
    "    indice=[]\n",
    "    for y in unique_labels:\n",
    "        indice_idxes=np.where(sub_targets==y)[0]\n",
    "        targets_idxes=indices[indice_idxes]\n",
    "        cur=int(ratio*len(indice_idxes))\n",
    "        used=used_label_num[y]\n",
    "        indice.extend(targets_idxes[used:used+cur])\n",
    "        used_label_num[y]+=cur\n",
    "    subsets.append(Subset(trainset, indice))\n",
    "subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 405,\n",
       "         1: 405,\n",
       "         2: 405,\n",
       "         3: 405,\n",
       "         4: 405,\n",
       "         5: 405,\n",
       "         6: 405,\n",
       "         7: 405,\n",
       "         8: 405,\n",
       "         9: 405,\n",
       "         10: 405,\n",
       "         11: 405,\n",
       "         12: 405,\n",
       "         13: 405,\n",
       "         14: 405,\n",
       "         15: 405,\n",
       "         16: 405,\n",
       "         17: 405,\n",
       "         18: 405,\n",
       "         19: 405,\n",
       "         20: 405,\n",
       "         21: 405,\n",
       "         22: 405,\n",
       "         23: 405,\n",
       "         24: 405,\n",
       "         25: 405,\n",
       "         26: 405,\n",
       "         27: 405,\n",
       "         28: 405,\n",
       "         29: 405,\n",
       "         30: 405,\n",
       "         31: 405,\n",
       "         32: 405,\n",
       "         33: 405,\n",
       "         34: 405,\n",
       "         35: 405,\n",
       "         36: 405,\n",
       "         37: 405,\n",
       "         38: 405,\n",
       "         39: 405,\n",
       "         40: 405,\n",
       "         41: 405,\n",
       "         42: 405,\n",
       "         43: 405,\n",
       "         44: 405,\n",
       "         45: 405,\n",
       "         46: 405,\n",
       "         47: 405,\n",
       "         48: 405,\n",
       "         49: 405,\n",
       "         50: 405,\n",
       "         51: 405,\n",
       "         52: 405,\n",
       "         53: 405,\n",
       "         54: 405,\n",
       "         55: 405,\n",
       "         56: 405,\n",
       "         57: 405,\n",
       "         58: 405,\n",
       "         59: 405,\n",
       "         60: 405,\n",
       "         61: 405,\n",
       "         62: 405,\n",
       "         63: 405,\n",
       "         64: 405,\n",
       "         65: 405,\n",
       "         66: 405,\n",
       "         67: 405,\n",
       "         68: 405,\n",
       "         69: 405,\n",
       "         70: 405,\n",
       "         71: 405,\n",
       "         72: 405,\n",
       "         73: 405,\n",
       "         74: 405,\n",
       "         75: 405,\n",
       "         76: 405,\n",
       "         77: 405,\n",
       "         78: 405,\n",
       "         79: 405,\n",
       "         80: 405,\n",
       "         81: 405,\n",
       "         82: 405,\n",
       "         83: 405,\n",
       "         84: 405,\n",
       "         85: 405,\n",
       "         86: 405,\n",
       "         87: 405,\n",
       "         88: 405,\n",
       "         89: 405,\n",
       "         90: 405,\n",
       "         91: 405,\n",
       "         92: 405,\n",
       "         93: 405,\n",
       "         94: 405,\n",
       "         95: 405,\n",
       "         96: 405,\n",
       "         97: 405,\n",
       "         98: 405,\n",
       "         99: 405})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "q,w=subsets\n",
    "i=q.indices\n",
    "d=np.array(q.dataset.targets)[i]\n",
    "Counter(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls50 = ['_R_P_SP','_C_L','_L','_C','_P_SP','_R_SP_N','_C_P','_P_W','_R_P','_P','_C_P_SP','_R_P_SP_N','_R','_C_SP_N','_W','_R_SP','_W_SP','_C_SP','_R_L','_W_N_A','_C_N','_P_A','_SP_N','_R_N','_W_A','_N','_P_SP_A','_P_SP_N_A','_N_A','_P_N','_V_N','_P_W_A','_P_W_N_A','_W_SP_N_A','_CA','_SP_N_A','_R_TEN','_C_A','_TEN','_R_V','_W_CA','_P_W_SP_N_A','_C_R','_P_W_SP_A','_S','_W_S','_P_N_A','_M_P','_M_SP','_V_P']\n",
    "cls10=['_R_P_SP', '_C_L', '_L', '_P_SP', '_R_SP_N', '_C_P', '_P_W', '_P','_C_P_SP', '_R_P_SP_N']\n",
    "\n",
    "cls_label={\n",
    "    'cls10':cls10,\n",
    "    'cls50':cls50\n",
    "}\n",
    "\n",
    "def read_line(line,cls_num):\n",
    "    if not line:\n",
    "        return\n",
    "    try:\n",
    "        data=json.loads(line)\n",
    "    except Exception as e:\n",
    "        return\n",
    "    labels=cls_label[cls_num]\n",
    "    label,d=list(data.items())[0]\n",
    "    if label not in labels:\n",
    "        return\n",
    "    _y=labels.index(label)\n",
    "    _x=[d['i'],d['r'],d['a']]\n",
    "    return _x,_y\n",
    "    \n",
    "def extract_json(filename,cls_num):\n",
    "    x,y=[],[]\n",
    "    with open(filename,encoding='UTF-8') as f:\n",
    "        for line in f.readlines():\n",
    "            ret=read_line(line,cls_num)\n",
    "            if not ret:\n",
    "                continue\n",
    "            x.append(ret[0])\n",
    "            y.append(ret[1])\n",
    "    return x,y\n",
    "\n",
    "def save_to_npy(x,y,filename):\n",
    "    np.save(filename+'_x.npy',x)\n",
    "    np.save(filename+'_y.npy',y)\n",
    "    \n",
    "def read_npy(filename):\n",
    "    x=np.load(filename+'_x.npy')\n",
    "    y=np.load(filename+'_y.npy')\n",
    "    return x,y\n",
    "\n",
    "x,y=extract_json('/root/autodl-nas/test.json','cls10')\n",
    "save_to_npy(x,y,'/root/autodl-nas/cls10_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 9481,\n",
       "         9: 2186,\n",
       "         6: 3154,\n",
       "         3: 6641,\n",
       "         8: 2310,\n",
       "         7: 2692,\n",
       "         4: 4435,\n",
       "         5: 3773,\n",
       "         2: 9408,\n",
       "         0: 5662})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_npy(x,y,'/root/autodl-nas/cls10_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'py38' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n py38 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'py38' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n py38 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0,p1,p2=torch.randn((100,10)),torch.randn((100,10)),torch.randn((100,10))\n",
    "h0,h1,h2 = F.softmax(p0,dim=1),F.softmax(p1,dim=1),F.softmax(p2,dim=1)\n",
    "prob0,label0=torch.max(h0,dim=1)\n",
    "prob1,label1=torch.max(h1,dim=1)\n",
    "prob2,label2=torch.max(h2,dim=1)\n",
    "\n",
    "sh=torch.zeros((3,10))\n",
    "shc=torch.zeros((3,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,prob,label in zip([0,1,2],[prob0,prob1,prob2],[label0,label1,label2]):\n",
    "    for i in range(10):\n",
    "        mask=torch.where(label==i)[0]\n",
    "        sh[idx,i]+=prob[mask].sum()\n",
    "        shc[idx,i]+=len(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.2687, 2.8858, 3.4718, 3.3235, 2.2586, 2.7701, 3.1534, 3.3735, 2.3487,\n",
       "          5.4121],\n",
       "         [3.8822, 2.7633, 3.9745, 2.9440, 1.8768, 3.7684, 3.8995, 2.4050, 3.8203,\n",
       "          2.4709],\n",
       "         [2.6963, 2.9775, 3.6299, 4.4795, 2.6984, 3.4844, 2.8621, 2.6690, 3.5187,\n",
       "          2.5244]]),\n",
       " tensor([[ 7., 10., 11., 11.,  7.,  9., 10., 11.,  9., 15.],\n",
       "         [12.,  8., 12.,  9.,  7., 11., 12.,  9., 12.,  8.],\n",
       "         [ 9.,  9., 11., 14.,  8., 11.,  9.,  9., 12.,  8.]]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh,shc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3241, 0.2886, 0.3156, 0.3021, 0.3227, 0.3078, 0.3153, 0.3067, 0.2610,\n",
       "         0.3608],\n",
       "        [0.3235, 0.3454, 0.3312, 0.3271, 0.2681, 0.3426, 0.3250, 0.2672, 0.3184,\n",
       "         0.3089],\n",
       "        [0.2996, 0.3308, 0.3300, 0.3200, 0.3373, 0.3168, 0.3180, 0.2966, 0.2932,\n",
       "         0.3156]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh/shc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.1398,  0.9709,  0.9804,  1.6643, -0.2535, -0.5178,  1.0996,  0.2675,\n",
       "          1.5618, -1.4261]),\n",
       " tensor([8, 4, 1, 9, 3, 1, 8, 3, 9, 7]),\n",
       " tensor([0.3241, 0.2886, 0.3156, 0.3021, 0.3227, 0.3078, 0.3153, 0.3067, 0.2610,\n",
       "         0.3608]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob,label=torch.randn(10)+(sh/shc)[0],torch.randint(0,10,(10,))\n",
    "prob,label,(sh/shc)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask1 tensor([False, False, False, False, False, False, False, False, False, False]) cls_prob tensor([])\n",
      "mask2 tensor([ True, False, False, False,  True,  True, False,  True, False,  True]) m1*m2 tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "mask1 tensor([False, False,  True, False, False,  True, False, False, False, False]) cls_prob tensor([ 0.9804, -0.5178])\n",
      "mask2 tensor([ True, False, False, False,  True,  True, False,  True, False,  True]) m1*m2 tensor([False, False, False, False, False,  True, False, False, False, False])\n",
      "mask1 tensor([False, False, False, False, False, False, False, False, False, False]) cls_prob tensor([])\n",
      "mask2 tensor([ True, False, False, False,  True,  True, False,  True, False,  True]) m1*m2 tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "mask1 tensor([False, False, False, False,  True, False, False,  True, False, False]) cls_prob tensor([-0.2535,  0.2675])\n",
      "mask2 tensor([ True, False, False, False,  True,  True, False,  True, False,  True]) m1*m2 tensor([False, False, False, False,  True, False, False,  True, False, False])\n",
      "mask1 tensor([False,  True, False, False, False, False, False, False, False, False]) cls_prob tensor([0.9709])\n",
      "mask2 tensor([ True, False, False, False,  True,  True, False,  True, False,  True]) m1*m2 tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "mask1 tensor([False, False, False, False, False, False, False, False, False, False]) cls_prob tensor([])\n",
      "mask2 tensor([ True, False, False, False,  True,  True, False,  True, False,  True]) m1*m2 tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "mask1 tensor([False, False, False, False, False, False, False, False, False, False]) cls_prob tensor([])\n",
      "mask2 tensor([ True, False, False, False,  True,  True, False,  True, False,  True]) m1*m2 tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "mask1 tensor([False, False, False, False, False, False, False, False, False,  True]) cls_prob tensor([-1.4261])\n",
      "mask2 tensor([ True, False, False, False,  True,  True, False,  True, False,  True]) m1*m2 tensor([False, False, False, False, False, False, False, False, False,  True])\n",
      "mask1 tensor([ True, False, False, False, False, False,  True, False, False, False]) cls_prob tensor([-1.1398,  1.0996])\n",
      "mask2 tensor([ True, False, False, False,  True,  True, False,  True, False,  True]) m1*m2 tensor([ True, False, False, False, False, False, False, False, False, False])\n",
      "mask1 tensor([False, False, False,  True, False, False, False, False,  True, False]) cls_prob tensor([1.6643, 1.5618])\n",
      "mask2 tensor([ True, False, False, False,  True,  True, False,  True, False,  True]) m1*m2 tensor([False, False, False, False, False, False, False, False, False, False])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.9709, 0.9804, 1.6643, 0.0000, 0.0000, 1.0996, 0.0000, 1.5618,\n",
       "        0.0000])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从tensor中随机抽取n个元素\n",
    "def random_sample(tensor,n):\n",
    "    idx=torch.randperm(tensor.size(0))[:n]\n",
    "    return tensor[idx]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "class ToTensor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, origin):\n",
    "        return torch.Tensor(origin).float()\n",
    "\n",
    "t=transforms.Compose([\n",
    "            ToTensor(),\n",
    "            ToTensor(),\n",
    "            ToTensor()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt=t.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Compose(\n",
       "     <__main__.ToTensor object at 0x7ff9fb96dfa0>\n",
       "     <__main__.ToTensor object at 0x7ff9fbb38760>\n",
       "     <__main__.ToTensor object at 0x7ff9fb9a4e20>\n",
       " ),\n",
       " Compose(\n",
       "     <__main__.ToTensor object at 0x7ff9fb96dfa0>\n",
       "     <__main__.ToTensor object at 0x7ff9fbb38760>\n",
       "     <__main__.ToTensor object at 0x7ff9fb9a4e20>\n",
       " ))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.Compose(tt),t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt[0](np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h=torch.zeros((3,10))/torch.zeros((3,10))\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[h.isnan()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.dtype' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m h[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m==\u001b[39mtorch\u001b[39m.\u001b[39;49mfloat(torch\u001b[39m.\u001b[39;49mnan)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'torch.dtype' object is not callable"
     ]
    }
   ],
   "source": [
    "h[0][0]==torch.float(torch.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8004ef2893cd8ba7bcbe56b84cb0d06ac01e074b775ed352f293a083b0ffbcee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
